{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 109, 'name': 'Wine', 'repository_url': 'https://archive.ics.uci.edu/dataset/109/wine', 'data_url': 'https://archive.ics.uci.edu/static/public/109/data.csv', 'abstract': 'Using chemical analysis to determine the origin of wines', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 178, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1992, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C5PC7J', 'creators': ['Stefan Aeberhard', 'M. Forina'], 'intro_paper': {'title': 'Comparative analysis of statistical pattern recognition methods in high dimensional settings', 'authors': 'S. Aeberhard, D. Coomans, O. Vel', 'published_in': 'Pattern Recognition', 'year': 1994, 'url': 'https://www.semanticscholar.org/paper/83dc3e4030d7b9fbdbb4bde03ce12ab70ca10528', 'doi': '10.1016/0031-3203(94)90145-7'}, 'additional_info': {'summary': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \\r\\n\\r\\nI think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.)  I lost it, and b.), I would not know which 13 variables are included in the set.\\r\\n\\r\\nThe attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it )\\r\\n1) Alcohol\\r\\n2) Malic acid\\r\\n3) Ash\\r\\n4) Alcalinity of ash  \\r\\n5) Magnesium\\r\\n6) Total phenols\\r\\n7) Flavanoids\\r\\n8) Nonflavanoid phenols\\r\\n9) Proanthocyanins\\r\\n10)Color intensity\\r\\n11)Hue\\r\\n12)OD280/OD315 of diluted wines\\r\\n13)Proline \\r\\n\\r\\nIn a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.           ', 'purpose': 'test', 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All attributes are continuous\\r\\n\\t\\r\\nNo statistics available, but suggest to standardise variables for certain uses (e.g. for us with classifiers which are NOT scale invariant)\\r\\n\\r\\nNOTE: 1st attribute is class identifier (1-3)', 'citation': None}}\n",
      "                            name     role         type demographic  \\\n",
      "0                          class   Target  Categorical        None   \n",
      "1                        Alcohol  Feature   Continuous        None   \n",
      "2                      Malicacid  Feature   Continuous        None   \n",
      "3                            Ash  Feature   Continuous        None   \n",
      "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
      "5                      Magnesium  Feature      Integer        None   \n",
      "6                  Total_phenols  Feature   Continuous        None   \n",
      "7                     Flavanoids  Feature   Continuous        None   \n",
      "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
      "9                Proanthocyanins  Feature   Continuous        None   \n",
      "10               Color_intensity  Feature   Continuous        None   \n",
      "11                           Hue  Feature   Continuous        None   \n",
      "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
      "13                       Proline  Feature      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "5         None  None             no  \n",
      "6         None  None             no  \n",
      "7         None  None             no  \n",
      "8         None  None             no  \n",
      "9         None  None             no  \n",
      "10        None  None             no  \n",
      "11        None  None             no  \n",
      "12        None  None             no  \n",
      "13        None  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine.data.features \n",
    "y = wine.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(wine.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1 DTEC algorithm\n",
    "# Require: Dataset X, number of clusters K (not obligatory).\n",
    "# Ensure: An unsupervised evidential decision tree T.\n",
    "# Initialize the root node of decision tree T using dataset X;\n",
    "# while there is unevaluated node of single cluster do\n",
    "# Evaluate all possible cutting points at the taken node by the evidential silhouette metric using Eqs. (4)-(7);\n",
    "# Select the cutting point with the largest average silhouette value;\n",
    "# if the average silhouette value after splitting is larger than before then\n",
    "# Split this node of single cluster using Eqs. (8)-(10);\n",
    "# Determine the boundaries of the generated child nodes;\n",
    "# Use these boundaries to split the node of meta-cluster which includes the above single cluster;\n",
    "# else\n",
    "# Go to next node;\n",
    "# end if\n",
    "# end while\n",
    "# while K is available and the number of generated clusters is not equal to K do\n",
    "# if the number of generated clusters is larger than K then\n",
    "# Evaluate the quality of each single cluster by the evidential silhouette metric using Eq. (11);\n",
    "# Merge the cluster having lowest quality with its nearest cluster;\n",
    "# else\n",
    "# Continue splitting at the leaf node that has the largest average evidential silhouette value after splitting.\n",
    "# end if\n",
    "# end while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the root node of decision tree T using dataset X;\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing root node of decision tree T using dataset X\n",
    "\n",
    "class DecisionNode:\n",
    "    \"\"\"Class to represent a decision node in a decision tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, decision_function, class_label=None):\n",
    "        \"\"\"Create a node with a left child, right child, decision function and optional class label.\n",
    "        This is a binary tree so each node has two children (left and right). \n",
    "        The decision function is used to make a decision when the node is asked to classify an instance.\n",
    "        \n",
    "        Args:\n",
    "            left (DecisionNode) : left child node\n",
    "            right (DecisionNode) : right child node\n",
    "            decision_function (function) : function to make decision\n",
    "            class_label (int) : optional class label for the node\n",
    "        \"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "        \n",
    "    def decide(self, feature):\n",
    "        \"\"\"Classify an instance based on its feature vector using the decision function.\"\"\"\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "        else:\n",
    "            return self.right.decide(feature)\n",
    "        \n",
    "\n",
    "# Pignistic probability BetP(A) = summation(|A âˆ© B|/|B|) . m(B) \n",
    "# where A is a subset of B, and m(B) is the mass function of B.\n",
    "# The pignistic probability is a measure of the belief in the proposition A given the evidence B.\n",
    "class DecisonTree:\n",
    "    \"\"\"Class to represent a decision tree model for classification.\"\"\"\n",
    "    \n",
    "    Dataset = \n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"Create a decision tree model.\n",
    "        \n",
    "        Args:\n",
    "            max_depth (int) : maximum depth of the tree\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree model by fitting to the data.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like) : feature vectors\n",
    "            y (array-like) : class labels\n",
    "        \"\"\"\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the decision tree model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like) : feature vectors\n",
    "            y (array-like) : class labels\n",
    "            depth (int) : current depth of the tree\n",
    "        \"\"\"\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return DecisionNode(None, None, None, class_label=self._majority_class(y))\n",
    "        \n",
    "    \n",
    "    def pignistic_probability(self, A, B, m):\n",
    "        \"\"\"Calculate the pignistic probability of A given B.\n",
    "        \n",
    "        Args:\n",
    "            A (array-like) : subset of B\n",
    "            B (array-like) : evidence\n",
    "            m (array-like) : mass function of B\n",
    "        \"\"\"\n",
    "        return sum(len(set(A) & set(B)) / len(B) * m[B] for B in B)\n",
    "    \n",
    "    def  lenSet(self, feature, l, r):\n",
    "        # return length of dataset that has the feature value in the range [l, r]\n",
    "        return len([x for x in feature if l <= x <= r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
