{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1 DTEC algorithm\n",
    "# Require: Dataset X, number of clusters K (not obligatory).\n",
    "# Ensure: An unsupervised evidential decision tree T.\n",
    "# Initialize the root node of decision tree T using dataset X;\n",
    "# while there is unevaluated node of single cluster do\n",
    "# Evaluate all possible cutting points at the taken node by the evidential silhouette metric using Eqs. (4)-(7);\n",
    "# Select the cutting point with the largest average silhouette value;\n",
    "# if the average silhouette value after splitting is larger than before then\n",
    "# Split this node of single cluster using Eqs. (8)-(10);\n",
    "# Determine the boundaries of the generated child nodes;\n",
    "# Use these boundaries to split the node of meta-cluster which includes the above single cluster;\n",
    "# else\n",
    "# Go to next node;\n",
    "# end if\n",
    "# end while\n",
    "# while K is available and the number of generated clusters is not equal to K do\n",
    "# if the number of generated clusters is larger than K then\n",
    "# Evaluate the quality of each single cluster by the evidential silhouette metric using Eq. (11);\n",
    "# Merge the cluster having lowest quality with its nearest cluster;\n",
    "# else\n",
    "# Continue splitting at the leaf node that has the largest average evidential silhouette value after splitting.\n",
    "# end if\n",
    "# end while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionNode:\n",
    "    \"\"\"Class to represent a decision node in a decision tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, mass_functions, decision_function, instance_indices, class_label=None):\n",
    "        \"\"\"Create a node with a left child, right child, decision function and optional class label.\n",
    "        This is a binary tree so each node has two children (left and right). \n",
    "        The decision function is used to make a decision when the node is asked to classify an instance.\n",
    "        \n",
    "        Args:\n",
    "            left (DecisionNode) : left child node\n",
    "            right (DecisionNode) : right child node\n",
    "            decision_function (function) : function to make decision\n",
    "            class_label (int) : optional class label for the node\n",
    "        \"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.mass_functions = mass_functions\n",
    "        self.instance_indices = instance_indices\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "        \n",
    "    def decide(self, feature):\n",
    "        \"\"\"Classify an instance based on its feature vector using the decision function.\"\"\"\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "        else:\n",
    "            return self.right.decide(feature)\n",
    "        \n",
    "\n",
    "# Pignistic probability BetP(A) = summation(|A âˆ© B|/|B|) . m(B) \n",
    "# where A is a subset of B, and m(B) is the mass function of B.\n",
    "# The pignistic probability is a measure of the belief in the proposition A given the evidence B.\n",
    "class DecisonTree:\n",
    "    \"\"\"Class to represent a decision tree model for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"Create a decision tree model.\n",
    "        \n",
    "        Args:\n",
    "            max_depth (int) : maximum depth of the tree\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.Dataset = fetch_ucirepo(id=109)\n",
    "        self.X = self.Dataset.data.features\n",
    "        #convert X to a numpy array\n",
    "        self.X = np.array(self.X)\n",
    "        self.y = self.Dataset.data.targets\n",
    "        #convert y to a numpy array\n",
    "        self.y = np.array(self.y)\n",
    "        self.mass_functions = {}\n",
    "        self.metadata = self.Dataset.metadata\n",
    "        self.variables = self.Dataset.variables\n",
    "        self.root_instance_indices = np.arange(len(self.X))\n",
    "        self.root = None  \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree model by fitting to the data.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like) : feature vectors\n",
    "            y (array-like) : class labels\n",
    "        \"\"\"\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the decision tree model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like) : feature vectors\n",
    "            y (array-like) : class labels\n",
    "            depth (int) : current depth of the tree\n",
    "        \"\"\"\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return DecisionNode(None, None, None, class_label=self._majority_class(y))\n",
    "    \n",
    "        \n",
    "    def feature_distance(self, xi, xj):\n",
    "        \"\"\"Calculate the distance between two feature vectors.\"\"\"\n",
    "        return np.sum((xi - xj) ** 2)\n",
    "    \n",
    "    def pignistic_probability_unit(self, A, B):\n",
    "        \"\"\"Calculate the pignistic probability of A given B.\n",
    "        \n",
    "        Args:\n",
    "            A (array-like) : subset of B\n",
    "            B (array-like) : evidence\n",
    "            m (array-like) : mass function of B\n",
    "        \"\"\"\n",
    "        return len(np.intersect1d(A, B)) / len(B)\n",
    "    \n",
    "    def cutting_points(self, feature):\n",
    "        \"\"\"Find all possible cutting points for a feature.\"\"\"\n",
    "        return np.unique(self.X[:, feature])\n",
    "    \n",
    "    def cut_feature(self, instance_indices, feature, cutting_point):\n",
    "        \"\"\"Split the dataset based on a feature and cutting point and return the indices of the points.\"\"\"\n",
    "        L = np.where(self.X[:, feature] < cutting_point)[0]\n",
    "        R = np.where(self.X[:, feature] > cutting_point)[0]\n",
    "        L = np.intersect1d(L, instance_indices)\n",
    "        R = np.intersect1d(R, instance_indices)\n",
    "        return L, R\n",
    "    \n",
    "    def calculate_centers(self, l, r, parent_mass):\n",
    "        \"\"\"Calculate the centers of the child nodes.\"\"\"\n",
    "        c_l = np.sum([self.X[i] * parent_mass[i] for i in l], axis=0) / np.sum([parent_mass[i] for i in l])\n",
    "        c_r = np.sum([self.X[i] * parent_mass[i] for i in r], axis=0) / np.sum([parent_mass[i] for i in r])\n",
    "        return c_l, c_r\n",
    "    \n",
    "    def calculate_mass_functions(self, instance_indices, cutting_point, c_l, c_r, gamma):\n",
    "        \"\"\"Calculate the mass function for the child nodes.\"\"\"\n",
    "        d_l = np.array([self.feature_distance(self.X[i], c_l) for i in instance_indices]) / self.feature_distance(c_l, cutting_point)\n",
    "        d_r = np.array([self.feature_distance(self.X[i], c_r) for i in instance_indices]) / self.feature_distance(c_r, cutting_point)\n",
    "        d_m = np.array([self.feature_distance(self.X[i], cutting_point) for i in instance_indices]) / (self.feature_distance(c_l, c_r) / gamma)\n",
    "        \n",
    "        m_l = d_l / (d_l + d_r + d_m)\n",
    "        m_r = d_r / (d_l + d_r + d_m)\n",
    "        m_m = d_m / (d_l + d_r + d_m)\n",
    "        \n",
    "        return m_l, m_r, m_m\n",
    "    \n",
    "    def assign_clusters(self, instance_indices, m_l, m_r, m_m):\n",
    "        \"\"\"Assign instances to clusters based on the mass functions.\"\"\"\n",
    "        all_mass_functions = np.array([m_l, m_r, m_m])\n",
    "        cluster_assignments = np.argmax(all_mass_functions, axis=0)\n",
    "        clusters = [instance_indices[cluster_assignments == i] for i in range(3)]\n",
    "        l = clusters[0]\n",
    "        r = clusters[1]\n",
    "        m = clusters[2]\n",
    "        return l, r, m\n",
    "    \n",
    "    def calculate_silhouette(self, l, r, m, m_l, m_r, m_m, c_l, c_r, cutting_point):\n",
    "        pignistic_probability_l = self.pignistic_probability_unit(l, l) * m_l + self.pignistic_probability_unit(l, r) * m_r + self.pignistic_probability_unit(l, m) * m_m\n",
    "        pignistic_probability_r = self.pignistic_probability_unit(r, r) * m_r + self.pignistic_probability_unit(r, l) * m_l + self.pignistic_probability_unit(r, m) * m_m\n",
    "        pignistic_probability_m = self.pignistic_probability_unit(m, m) * m_m + self.pignistic_probability_unit(m, l) * m_m + self.pignistic_probability_unit(m, r) * m_r\n",
    "                \n",
    "        a_l = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probability_l[j]) for j in l]) for i in l) / (np.sum([pignistic_probability_l[j] for j in l]))])\n",
    "        a_r = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probability_r[j]) for j in r]) for i in r) / (np.sum([pignistic_probability_r[j] for j in r]))])\n",
    "        a_m = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probability_m[j]) for j in m]) for i in m) / (np.sum([pignistic_probability_m[j] for j in m]))])\n",
    "\n",
    "        clusters = [l, r, m]\n",
    "        pignistic_probabilities = [pignistic_probability_l, pignistic_probability_r, pignistic_probability_m]\n",
    "\n",
    "        n_l = 0\n",
    "        if self.feature_distance(c_l, c_r) > self.feature_distance(c_l, cutting_point):\n",
    "            n_l = 2\n",
    "        else:\n",
    "            n_l = 1\n",
    "\n",
    "        n_r = 0\n",
    "        if self.feature_distance(c_r, c_l) > self.feature_distance(c_r, cutting_point):\n",
    "            n_r = 2\n",
    "        else:\n",
    "            n_r = 0\n",
    "\n",
    "        n_m = 0\n",
    "        if self.feature_distance(cutting_point, c_l) > self.feature_distance(cutting_point, c_r):\n",
    "            n_m = 1\n",
    "        else:\n",
    "            n_m = 0\n",
    "\n",
    "        b_l = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probabilities[n_l][j]) for j in clusters[n_l]]) for i in l) / (np.sum([pignistic_probabilities[n_l][j] for j in clusters[n_l]]))])\n",
    "        b_r = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probabilities[n_r][j]) for j in clusters[n_r]]) for i in r) / (np.sum([pignistic_probabilities[n_r][j] for j in clusters[n_r]]))])\n",
    "        b_m = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probabilities[n_m][j]) for j in clusters[n_m]]) for i in m) / (np.sum([pignistic_probabilities[n_m][j] for j in clusters[n_m]]))])\n",
    "        \n",
    "        es_l = (b_l - a_l) / np.maximum(a_l, b_l)\n",
    "        es_r = (b_r - a_r) / np.maximum(a_r, b_r)\n",
    "        es_m = (b_m - a_m) / np.maximum(a_m, b_m)\n",
    "\n",
    "        max_pignistic_probability = np.maximum(pignistic_probability_l, pignistic_probability_r, pignistic_probability_m)\n",
    "        \n",
    "        \n",
    "        num_sum = 0\n",
    "        den_sum = 0\n",
    "        for i in range(len(es_l)):\n",
    "            num_sum += max_pignistic_probability[l[i]] * es_l[i]\n",
    "            den_sum += max_pignistic_probability[l[i]]\n",
    "        for i in range(len(es_r)):\n",
    "            num_sum += max_pignistic_probability[r[i]] * es_r[i]\n",
    "            den_sum += max_pignistic_probability[r[i]]\n",
    "        for i in range(len(es_m)):\n",
    "            num_sum += max_pignistic_probability[m[i]] * es_m[i]\n",
    "            den_sum += max_pignistic_probability[m[i]]\n",
    "        \n",
    "        average_silhouette = num_sum / den_sum\n",
    "        \n",
    "        return average_silhouette      \n",
    "    \n",
    "    def max_silhouette(self, instance_indices, feature):\n",
    "        \"\"\"Find the cutting point with the largest average silhouette value.\"\"\"\n",
    "        cutting_points = self.cutting_points(feature)\n",
    "        max_silhouette = -np.inf\n",
    "        best_cutting_point = None\n",
    "        for cutting_point in cutting_points:\n",
    "            l, r = self.cut_feature(instance_indices, feature, cutting_point)\n",
    "            c_l, c_r = self.calculate_centers(l, r, instance_indices)\n",
    "            m_l, m_r, m_m = self.calculate_mass_functions(instance_indices, cutting_point, c_l, c_r, gamma=0.5)\n",
    "            l, r, m = self.assign_clusters(instance_indices, m_l, m_r, m_m)\n",
    "            silhouette = self.calculate_silhouette(l, r, m, m_l, m_r, m_m, c_l, c_r, cutting_point)\n",
    "            if silhouette > max_silhouette:\n",
    "                max_silhouette = silhouette\n",
    "                best_cutting_point = cutting_point\n",
    "        return best_cutting_point, max_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "Dataset = fetch_ucirepo(id=109)\n",
    "X = Dataset.data.features\n",
    "#convert X to a numpy array\n",
    "X = np.array(X)\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(X[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(X[:, 0] < 15.67)[0])\n",
    "print(X[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
