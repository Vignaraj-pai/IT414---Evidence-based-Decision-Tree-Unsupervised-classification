{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1 DTEC algorithm\n",
    "# Require: Dataset X, number of clusters K (not obligatory).\n",
    "# Ensure: An unsupervised evidential decision tree T.\n",
    "# Initialize the root node of decision tree T using dataset X;\n",
    "# while there is unevaluated node of single cluster do\n",
    "# Evaluate all possible cutting points at the taken node by the evidential silhouette metric using Eqs. (4)-(7);\n",
    "# Select the cutting point with the largest average silhouette value;\n",
    "# if the average silhouette value after splitting is larger than before then\n",
    "# Split this node of single cluster using Eqs. (8)-(10);\n",
    "# Determine the boundaries of the generated child nodes;\n",
    "# Use these boundaries to split the node of meta-cluster which includes the above single cluster;\n",
    "# else\n",
    "# Go to next node;\n",
    "# end if\n",
    "# end while\n",
    "# while K is available and the number of generated clusters is not equal to K do\n",
    "# if the number of generated clusters is larger than K then\n",
    "# Evaluate the quality of each single cluster by the evidential silhouette metric using Eq. (11);\n",
    "# Merge the cluster having lowest quality with its nearest cluster;\n",
    "# else\n",
    "# Continue splitting at the leaf node that has the largest average evidential silhouette value after splitting.\n",
    "# end if\n",
    "# end while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionNode:\n",
    "    \"\"\"Class to represent a decision node in a decision tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, left, right, mass_functions, decision_function, instance_indices, class_label=None):\n",
    "        \"\"\"Create a node with a left child, right child, decision function and optional class label.\n",
    "        This is a binary tree so each node has two children (left and right). \n",
    "        The decision function is used to make a decision when the node is asked to classify an instance.\n",
    "        \n",
    "        Args:\n",
    "            left (DecisionNode) : left child node\n",
    "            right (DecisionNode) : right child node\n",
    "            decision_function (function) : function to make decision\n",
    "            class_label (int) : optional class label for the node\n",
    "        \"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.mass_functions = mass_functions\n",
    "        self.instance_indices = instance_indices\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "        \n",
    "    def decide(self, feature):\n",
    "        \"\"\"Classify an instance based on its feature vector using the decision function.\"\"\"\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "        else:\n",
    "            return self.right.decide(feature)\n",
    "        \n",
    "\n",
    "# Pignistic probability BetP(A) = summation(|A âˆ© B|/|B|) . m(B) \n",
    "# where A is a subset of B, and m(B) is the mass function of B.\n",
    "# The pignistic probability is a measure of the belief in the proposition A given the evidence B.\n",
    "class DecisonTree:\n",
    "    \"\"\"Class to represent a decision tree model for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"Create a decision tree model.\n",
    "        \n",
    "        Args:\n",
    "            max_depth (int) : maximum depth of the tree\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.Dataset = fetch_ucirepo(id=109)\n",
    "        self.X = self.Dataset.data.features\n",
    "        #convert X to a numpy array\n",
    "        self.X = np.array(self.X)\n",
    "        self.y = self.Dataset.data.targets\n",
    "        #convert y to a numpy array\n",
    "        self.y = np.array(self.y)\n",
    "        self.mass_functions = {}\n",
    "        self.metadata = self.Dataset.metadata\n",
    "        self.variables = self.Dataset.variables\n",
    "        self.root = None  \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree model by fitting to the data.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like) : feature vectors\n",
    "            y (array-like) : class labels\n",
    "        \"\"\"\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the decision tree model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like) : feature vectors\n",
    "            y (array-like) : class labels\n",
    "            depth (int) : current depth of the tree\n",
    "        \"\"\"\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return DecisionNode(None, None, None, class_label=self._majority_class(y))\n",
    "    \n",
    "        \n",
    "    def feature_distance(self, xi, xj):\n",
    "        \"\"\"Calculate the distance between two feature vectors.\"\"\"\n",
    "        return np.sum((xi - xj) ** 2)\n",
    "    \n",
    "    def pignistic_probability_unit(self, A, B):\n",
    "        \"\"\"Calculate the pignistic probability of A given B.\n",
    "        \n",
    "        Args:\n",
    "            A (array-like) : subset of B\n",
    "            B (array-like) : evidence\n",
    "            m (array-like) : mass function of B\n",
    "        \"\"\"\n",
    "        return len(set(A) & set(B)) / len(B)\n",
    "    \n",
    "    def cutting_points(self, feature):\n",
    "        \"\"\"Find all possible cutting points for a feature.\"\"\"\n",
    "        return np.unique(self.X[:, feature])\n",
    "    \n",
    "    def cut_feature(self, instance_indices, feature, cutting_point):\n",
    "        \"\"\"Split the dataset based on a feature and cutting point and return the indices of the points.\"\"\"\n",
    "        L = np.where(self.X[:, feature] < cutting_point)[0]\n",
    "        R = np.where(self.X[:, feature] > cutting_point)[0]\n",
    "        L = np.intersect1d(L, instance_indices)\n",
    "        R = np.intersect1d(R, instance_indices)\n",
    "        return L, R\n",
    "    \n",
    "    def calculate_centers(self, l, r, parent_mass):\n",
    "        \"\"\"Calculate the centers of the child nodes.\"\"\"\n",
    "        c_l = np.sum([self.X[i] * parent_mass[i] for i in l], axis=0) / np.sum([parent_mass[i] for i in l])\n",
    "        c_r = np.sum([self.X[i] * parent_mass[i] for i in r], axis=0) / np.sum([parent_mass[i] for i in r])\n",
    "        return c_l, c_r\n",
    "    \n",
    "    def calculate_mass_functions(self, instance_indices, cutting_point, c_l, c_r, gamma):\n",
    "        \"\"\"Calculate the mass function for the child nodes.\"\"\"\n",
    "        d_l = np.array([self.feature_distance(self.X[i], c_l) for i in instance_indices]) / self.feature_distance(c_l, cutting_point)\n",
    "        d_r = np.array([self.feature_distance(self.X[i], c_r) for i in instance_indices]) / self.feature_distance(c_r, cutting_point)\n",
    "        d_m = np.array([self.feature_distance(self.X[i], cutting_point) for i in instance_indices]) / (self.feature_distance(c_l, c_r) / gamma)\n",
    "        \n",
    "        m_l = d_l / (d_l + d_r + d_m)\n",
    "        m_r = d_r / (d_l + d_r + d_m)\n",
    "        m_m = d_m / (d_l + d_r + d_m)\n",
    "        \n",
    "        return m_l, m_r, m_m\n",
    "    \n",
    "    def assign_clusters(self, instance_indices, m_l, m_r, m_m):\n",
    "        \"\"\"Assign instances to clusters based on the mass functions.\"\"\"\n",
    "        all_mass_functions = np.array([m_l, m_r, m_m])\n",
    "        cluster_assignments = np.argmax(all_mass_functions, axis=0)\n",
    "        clusters = [instance_indices[cluster_assignments == i] for i in range(3)]\n",
    "        l = clusters[0]\n",
    "        r = clusters[1]\n",
    "        m = clusters[2]\n",
    "        return l, r, m\n",
    "    \n",
    "    def calculate_silhouette(self, l, r, m, m_l, m_r, m_m):\n",
    "        pignistic_probability_l = self.pignistic_probability_unit(l, l)\n",
    "        pignistic_probability_r = self.pignistic_probability_unit(r, r)\n",
    "        pignistic_probability_m = self.pignistic_probability_unit(m, m)\n",
    "        pignistic_probability_l_r = self.pignistic_probability_unit(l, r)\n",
    "        pignistic_probability_l_m = self.pignistic_probability_unit(l, m)\n",
    "        pignistic_probability_r_m = self.pignistic_probability_unit(r, m)\n",
    "                \n",
    "        a_l = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * pignistic_probability_l * m_l[j] for j in l]) for i in l) / (np.sum([pignistic_probability_l * m_l[j] for j in l]))])\n",
    "        a_r = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * pignistic_probability_r * m_r[j] for j in r]) for i in r) / (np.sum([pignistic_probability_r * m_r[j] for j in r]))])\n",
    "        a_m = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * pignistic_probability_m * m_m[j] for j in m]) for i in m) / (np.sum([pignistic_probability_m * m_m[j] for j in m]))])\n",
    "        \n",
    "        b_l = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probability_l_r * m_r[j] + pignistic_probability_l_m * m_m[j]) for j in l]) for i in l) / (np.sum([pignistic_probability_l_r * m_r[j] + pignistic_probability_l_m * m_m[j] for j in l]))])\n",
    "        b_r = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probability_l_r * m_l[j] + pignistic_probability_r_m * m_m[j]) for j in r]) for i in r) / (np.sum([pignistic_probability_l_r * m_l[j] + pignistic_probability_r_m * m_m[j] for j in r]))])\n",
    "        b_m = np.array([(np.sum([self.feature_distance(self.X[i], self.X[j]) * (pignistic_probability_l_m * m_l[j] + pignistic_probability_r_m * m_r[j]) for j in m]) for i in m) / (np.sum([pignistic_probability_l_m * m_l[j] + pignistic_probability_r_m * m_r[j] for j in m]))])\n",
    "        \n",
    "        es_l = (b_l - a_l) / np.maximum(a_l, b_l)\n",
    "        es_r = (b_r - a_r) / np.maximum(a_r, b_r)\n",
    "        es_m = (b_m - a_m) / np.maximum(a_m, b_m)\n",
    "        \n",
    "        combined_es = np.concatenate((es_l, es_r, es_m))\n",
    "        \n",
    "        \n",
    "        average_silhouette = np.sum(max((pignistic_probability_l * m_l[i] + pignistic_probability_l_m * m_m[i] + pignistic_probability_l_r * m_r[i]), (pignistic_probability_r * m_r[i] + pignistic_probability_r_m * m_m[i] + pignistic_probability_l_r * m_l[i]), (pignistic_probability_m * m_m[i] + pignistic_probability_l_m * m_l[i] + pignistic_probability_r_m * m_r[i])) * combined_es[i] for i in range(len(combined_es))) / np.sum(max((pignistic_probability_l * m_l[i] + pignistic_probability_l_m * m_m[i] + pignistic_probability_l_r * m_r[i]), (pignistic_probability_r * m_r[i] + pignistic_probability_r_m * m_m[i] + pignistic_probability_l_r * m_l[i]), (pignistic_probability_m * m_m[i] + pignistic_probability_l_m * m_l[i] + pignistic_probability_r_m * m_r[i])) for i in range(len(combined_es)))\n",
    "        \n",
    "        return average_silhouette\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "Dataset = fetch_ucirepo(id=109)\n",
    "X = Dataset.data.features\n",
    "#convert X to a numpy array\n",
    "X = np.array(X)\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.03 11.41 11.45 11.46 11.56 11.61 11.62 11.64 11.65 11.66 11.76 11.79\n",
      " 11.81 11.82 11.84 11.87 11.96 12.   12.04 12.07 12.08 12.16 12.17 12.2\n",
      " 12.21 12.22 12.25 12.29 12.33 12.34 12.36 12.37 12.42 12.43 12.45 12.47\n",
      " 12.51 12.52 12.53 12.58 12.6  12.64 12.67 12.69 12.7  12.72 12.77 12.79\n",
      " 12.81 12.82 12.84 12.85 12.86 12.87 12.88 12.93 12.96 12.99 13.03 13.05\n",
      " 13.07 13.08 13.11 13.16 13.17 13.2  13.23 13.24 13.27 13.28 13.29 13.3\n",
      " 13.32 13.34 13.36 13.39 13.4  13.41 13.45 13.48 13.49 13.5  13.51 13.52\n",
      " 13.56 13.58 13.62 13.63 13.64 13.67 13.68 13.69 13.71 13.72 13.73 13.74\n",
      " 13.75 13.76 13.77 13.78 13.82 13.83 13.84 13.86 13.87 13.88 13.9  13.94\n",
      " 14.02 14.06 14.1  14.12 14.13 14.16 14.19 14.2  14.21 14.22 14.23 14.3\n",
      " 14.34 14.37 14.38 14.39 14.75 14.83]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(X[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(X[:, 0] < 15.67)[0])\n",
    "print(X[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
